
import { defer, Deferred } from './defer';

interface BatchDict<Params, Result> {
	[batchKey: string]: Batch<Params, Result>;
}

interface DeferredDict<T> {
	[requestKey: string]: Deferred<T>
}

interface BatchRunner<Params, Result> {
	(requests: RequestWrapper<Params>[]): Promise<BatchResult<Result>>;
}

/**
 * The value passed into `makeRequest()` for a given requested item.
 *
 * @param Params The parameters type for the batcher
 */
export interface RequestWrapper<Params> {
	/** The key generated by `getRequestKey()` */
	key: string | number;
	/** The parameters passed in from the `request()` call */
	params: Params;
}

/**
 * The results from a `makeRequest()` call, should be a dictionary of request keys
 * generated by `getRequestKey()` to the resulting value from the batcher
 *
 * @param Result The results type for the batcher
 */
export interface BatchResult<Result> {
	[requestKey: string]: Result;
}

/**
 * Defines a new batched request process. This class should be inherited into sub-classes
 * for each different type of batch process.
 *
 * At a minimum, you must define a `makeRequest` method, which actually performs the batch
 * operation. You can optionally also define a `getBatchKey` method to control how requests
 * are broken down into batches, and a `getRequestKey` method to define how request deduplication
 * is handled.
 *
 * @param Params The parameter type for individual requests. This is what you pass into
 *   the `request` method when adding something to a batch
 * @param Result The result for a single requested item. This is what you get back from
 *   the promise returned from the `request` method.
 */
export abstract class Batcher<Params, Result> {
	/**
	 * The dictionary of active batches being built
	 */
	protected readonly batches: BatchDict<Params, Result> = { };

	/**
 	 * @param maxSize The maximum number of requests that will go into a single batch
 	 * @param maxWait The maximum amount of time (in ms) to wait before running a batch
	 */
	constructor(
		protected readonly maxSize: number,
		protected readonly maxWait: number
	) { }

	/**
	 * Determines the batch key used for breaking up requests into separate batches. If not
	 * overridden by the child class, this defaults to always returning an empty string (meaning
	 * any request can be batched with any other request).
	 *
	 * @param params The request to be assigned to a batch
	 */
	protected getBatchKey(params: Params) : string | number {
		return '';
	}

	/**
	 * Determines the request key used for de-duplicating concurrent requests. If not overriden
	 * by the child class, this defaults to just returning `String(params)`.
	 *
	 * @param params The request to generate a key for
	 */
	protected getRequestKey(params: Params) : string | number {
		return String(params);
	}

	/**
	 * Actually makes a request for a fully formed batch. This method will receive an array of
	 * requested items to process, and should return a dictionary of results. Each request item
	 * is an object containing the request key and the params provided to `request()`. The result
	 * dictionary should be a map of request keys to result values.
	 *
	 * For example, if the request was made like this:
	 *
	 * ```
	 * myBatcher.request({ id: 123, name: 'bob' });
	 * ```
	 *
	 * The cooresponding request object might look like this:
	 *
	 * 
	 * ```
	 * {
	 *   key: "123",
	 *   params: {
	 *     id: 123,
	 *     name: 'bob'
	 *   }
	 * }
	 * ```
	 *
	 * And the results dictionary would look like this:
	 *
	 * ```
	 * {
	 *   "123": "some result here"
	 * }
	 * ```
	 *
	 * @param requests An array of all the requests in this batch
	 */
	protected abstract makeRequest(requests: RequestWrapper<Params>[]) : Promise<BatchResult<Result>>;

	/**
	 * Adds a request to be run in the next available batch. Resolves with the final result
	 * for this request once the batch runs
	 *
	 * @param params The request to be made
	 */
	public request(params: Params) : Promise<Result> {
		const batchKey = this.getBatchKey(params);
		const requestKey = this.getRequestKey(params);

		if (! this.batches[batchKey] || this.batches[batchKey].isClosed) {
			const runBatch = (requests: RequestWrapper<Params>[]) => {
				return this.makeRequest(requests);
			};

			this.batches[batchKey] = new Batch(this.maxSize, this.maxWait, runBatch);
		}

		return this.batches[batchKey].request(requestKey, params);
	}
}

class Batch<Params, Result> {
	protected timeout: number;
	protected requests: RequestWrapper<Params>[] = new Array(this.maxSize);
	protected nextIndex: number = 0;
	protected deferreds: DeferredDict<Result> = { };
	
	/** Is set to true once the batch starts processing to prevent new items from being added */
	public isClosed: boolean = false;

	constructor(
		protected readonly maxSize: number,
		protected readonly maxWait: number,
		protected runBatch: BatchRunner<Params, Result>
	) {
		this.timeout = setTimeout(() => this.run(), maxWait) as any;
	}

	protected async run() {
		if (this.isClosed) {
			return;
		}

		// Cancel the timeout in case it hasn't fired yet
		clearTimeout(this.timeout as any);

		// Close the batch to prevent anything else from happening here
		this.isClosed = true;

		const count = this.nextIndex;

		// If the batch was never filled, slice out only the indexes that actually have items
		const requests = count >= this.maxSize
			? this.requests
			: this.requests.slice(0, count);

		try {
			// Actually run the batch operation
			const results = await this.runBatch(requests);

			// Resolve all of the waiting deferreds with their results
			for (let i = 0; i < count; i++) {
				const key = requests[i].key;

				this.deferreds[key].resolve(results[key]);
			}
		}

		// In the case of an error, reject all of the waiting deferreds
		catch (error) {
			for (let i = 0; i < count; i++) {
				this.deferreds[requests[i].key].reject(error);
			}
		}

		// Cleanup the batch object to ease garbage collection now that we're done
		this.cleanup();
	}

	request(key: string | number, params: Params) : Promise<Result> {
		// If there is already an item in this batch that matches the key, reuse the promise
		if (this.deferreds[key]) {
			return this.deferreds[key].promise;
		}

		const deferred: Deferred<Result> = defer();

		// Add the new request to the batch
		this.requests[this.nextIndex++] = { key, params };

		// Add the new deferred to the waiting list
		this.deferreds[key] = deferred;

		// If this request pushed us to the max size, run the batch now
		if (this.nextIndex >= this.maxSize) {
			this.run();
		}

		return deferred.promise;
	}

	protected cleanup() {
		this.timeout = null;
		this.requests = null;
		this.nextIndex = null;
		this.deferreds = null;
		this.runBatch = null;
	}
}
